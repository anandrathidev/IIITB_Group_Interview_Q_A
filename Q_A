
Q.1 did you use Dimensionality reduction technique , what , how  [ PCA , was already discussed]
A. cor matrix, 

Q.2 Advantage of Lasso Over Ridge [ so i figured out aftyer some google Lasso can double as Dim redux ]

Q.3 In your project, Did you create additional feature, if so how and why

Q.4 SVM regularization .. fully grilled here, with ref to R implementation 

Q.5 logistic regression .. explain all formulas and meanings 

1] odds ration
2] log of odds ration
3] loss function etc 


Q.6 in NN, how to avoid vanishing gradient 

Q.6.2 in RNN, how to avoid exploding gradient 
use LTSM memory gates 

Q.7 How did i decided K in my K-means clustering [ scatter plot etc ]

Q.8 Liner regression formula for y = mx +c 

1] 
y = mx + c
m = (n*sum(XiYi) - sum(Xi)*sum(Yi)) /  (n * sum(Xi^2) -  sum(Xi) ^2 )
c =  ( sum(Yi) * sum(Xi^2) - sum(Xi)* sum(Xi * Yi) ) / (n * sum(Xi^2) -  sum(Xi) ^2 )

2] β=(X'X)^−1 X'Y

3] Gradient Descent 
use GD over analytical solution if:

you are considering changes in the model, generalizations, adding some more complex terms/regularization/modifications
you need a generic method because you do not know much about the future of the code and the model (you are only one of the developers)
analytical solution is more expensive computationaly, and you need efficiency
analytical solution requires more memory, which you do not have
analytical solution is hard to implement and you need easy, simple code

Risk of meeting different local optimum
The linear regression cost function is always a convex function - always has a single minimum
Bowl shaped
One global optima
So gradient descent will always converge to global optima
